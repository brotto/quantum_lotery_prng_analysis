#!/usr/bin/env python3
"""
Analisador Mestre Abrangente
Coordena todas as an√°lises e gera relat√≥rio final pormenorizado
"""

import numpy as np
import pandas as pd
from datetime import datetime, timedelta
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import json
import os
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# Importar m√≥dulos desenvolvidos
from seed_discovery_engine import SeedDiscoveryEngine
from temporal_change_detector import TemporalChangeDetector
from quantum_prng_analyzer import QuantumPRNGAnalyzer
from multi_prng_reverse_engineer import MultiPRNGReverseEngineer
from genetic_optimization_engine import GeneticOptimizationEngine

class ComprehensiveMasterAnalyzer:
    def __init__(self, data_path):
        self.data_path = data_path
        self.engine = None
        self.historical_data = None
        
        # M√≥dulos de an√°lise
        self.temporal_detector = None
        self.quantum_analyzer = None
        self.reverse_engineer = None
        self.genetic_optimizer = None
        
        # Resultados consolidados
        self.consolidated_results = {}
        self.master_report = {}
        self.confidence_scores = {}
        
        # Configura√ß√£o
        self.output_dir = Path("../output")
        self.output_dir.mkdir(exist_ok=True)
        
        # Timestamp para esta execu√ß√£o
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
    def execute_comprehensive_analysis(self):
        """Executa an√°lise abrangente completa"""
        print("üéØ INICIANDO AN√ÅLISE MESTRE ABRANGENTE")
        print("="*80)
        
        try:
            # 1. Inicializa√ß√£o
            self.initialize_system()
            
            # 2. An√°lise Temporal
            self.execute_temporal_analysis()
            
            # 3. An√°lise Qu√¢ntica
            self.execute_quantum_analysis()
            
            # 4. Engenharia Reversa
            self.execute_reverse_engineering()
            
            # 5. Otimiza√ß√£o Gen√©tica
            self.execute_genetic_optimization()
            
            # 6. Consolida√ß√£o e Correla√ß√£o
            self.consolidate_all_results()
            
            # 7. An√°lise de Confian√ßa
            self.calculate_confidence_scores()
            
            # 8. Gera√ß√£o de Relat√≥rio Mestre
            self.generate_master_report()
            
            # 9. Visualiza√ß√µes Interativas
            self.create_interactive_visualizations()
            
            # 10. Refinamento e Recomenda√ß√µes
            self.generate_final_recommendations()
            
            print("\n‚úÖ AN√ÅLISE MESTRE COMPLETA!")
            
        except Exception as e:
            print(f"\n‚ùå Erro durante an√°lise mestre: {e}")
            import traceback
            traceback.print_exc()
    
    def initialize_system(self):
        """Inicializa o sistema e carrega dados"""
        print("\nüîß FASE 1: Inicializa√ß√£o do Sistema")
        print("-" * 50)
        
        # Inicializar engine principal
        self.engine = SeedDiscoveryEngine()
        
        # Carregar dados
        print("üìÇ Carregando dados da Mega Sena...")
        self.historical_data = self.engine.load_megasena_data(self.data_path)
        
        if not self.historical_data:
            raise ValueError("Falha ao carregar dados hist√≥ricos")
        
        print(f"‚úÖ {len(self.historical_data)} sorteios carregados com sucesso")
        
        # Inicializar m√≥dulos de an√°lise
        print("üî® Inicializando m√≥dulos de an√°lise...")
        
        self.temporal_detector = TemporalChangeDetector(self.historical_data)
        self.quantum_analyzer = QuantumPRNGAnalyzer(self.historical_data)
        self.reverse_engineer = MultiPRNGReverseEngineer(self.historical_data)
        
        print("‚úÖ Sistema inicializado com sucesso")
    
    def execute_temporal_analysis(self):
        """Executa an√°lise temporal completa"""
        print("\n‚è∞ FASE 2: An√°lise Temporal Avan√ßada")
        print("-" * 50)
        
        try:
            # Extrair features
            print("üî¨ Extraindo features temporais...")
            self.temporal_detector.extract_comprehensive_features()
            
            # Detectar pontos de mudan√ßa
            print("üîç Detectando pontos de mudan√ßa...")
            change_points = self.temporal_detector.detect_change_points_multiple_methods()
            
            # Analisar regimes
            print("üìã Analisando caracter√≠sticas dos regimes...")
            regimes = self.temporal_detector.analyze_regime_characteristics()
            
            # Gerar relat√≥rio temporal
            temporal_report = self.temporal_detector.generate_temporal_report()
            
            # Visualiza√ß√µes
            self.temporal_detector.visualize_temporal_analysis()
            
            # Armazenar resultados
            self.consolidated_results['temporal'] = {
                'change_points': change_points,
                'regimes': regimes,
                'report': temporal_report,
                'status': 'completed'
            }
            
            print(f"‚úÖ An√°lise temporal completa - {len(change_points)} pontos de mudan√ßa detectados")
            
        except Exception as e:
            print(f"‚ùå Erro na an√°lise temporal: {e}")
            self.consolidated_results['temporal'] = {'status': 'failed', 'error': str(e)}
    
    def execute_quantum_analysis(self):
        """Executa an√°lise qu√¢ntica completa"""
        print("\nüåå FASE 3: An√°lise Qu√¢ntica de PRNGs")
        print("-" * 50)
        
        try:
            # An√°lise de entropia qu√¢ntica
            print("üí´ Executando an√°lise de entropia qu√¢ntica...")
            self.quantum_analyzer.quantum_entropy_analysis()
            
            # An√°lise de Fourier qu√¢ntica
            print("üîÑ Executando an√°lise de Fourier qu√¢ntica...")
            self.quantum_analyzer.quantum_fourier_analysis()
            
            # Detec√ß√£o de PRNG qu√¢ntica
            print("üéØ Executando detec√ß√£o qu√¢ntica de PRNG...")
            prng_candidates = self.quantum_analyzer.quantum_prng_detection()
            
            # Otimiza√ß√£o qu√¢ntica
            print("üîç Executando busca com otimiza√ß√£o qu√¢ntica...")
            self.quantum_analyzer.quantum_optimization_search()
            
            # An√°lise de entrela√ßamento
            print("üîó Executando an√°lise de entrela√ßamento...")
            self.quantum_analyzer.quantum_entanglement_analysis()
            
            # An√°lise de coer√™ncia
            print("üí´ Executando an√°lise de coer√™ncia...")
            self.quantum_analyzer.quantum_coherence_analysis()
            
            # Gerar relat√≥rio qu√¢ntico
            quantum_report = self.quantum_analyzer.generate_quantum_report()
            
            # Visualiza√ß√µes
            self.quantum_analyzer.visualize_quantum_analysis()
            
            # Armazenar resultados
            self.consolidated_results['quantum'] = {
                'prng_candidates': prng_candidates,
                'report': quantum_report,
                'analysis_results': self.quantum_analyzer.quantum_analysis_results,
                'status': 'completed'
            }
            
            print(f"‚úÖ An√°lise qu√¢ntica completa - {len(prng_candidates)} candidatos PRNG detectados")
            
        except Exception as e:
            print(f"‚ùå Erro na an√°lise qu√¢ntica: {e}")
            self.consolidated_results['quantum'] = {'status': 'failed', 'error': str(e)}
    
    def execute_reverse_engineering(self):
        """Executa engenharia reversa completa"""
        print("\nüîß FASE 4: Engenharia Reversa Multi-PRNG")
        print("-" * 50)
        
        try:
            # Executar engenharia reversa para todos os PRNGs
            print("üîç Executando engenharia reversa...")
            reverse_results = self.reverse_engineer.reverse_engineer_all_prngs()
            
            # Gerar relat√≥rio de engenharia reversa
            reverse_report = self.reverse_engineer.generate_comprehensive_report()
            
            # Armazenar resultados
            self.consolidated_results['reverse_engineering'] = {
                'analysis_results': reverse_results,
                'prng_candidates': self.reverse_engineer.prng_candidates,
                'report': reverse_report,
                'status': 'completed'
            }
            
            print(f"‚úÖ Engenharia reversa completa - {len(self.reverse_engineer.prng_candidates)} candidatos identificados")
            
        except Exception as e:
            print(f"‚ùå Erro na engenharia reversa: {e}")
            self.consolidated_results['reverse_engineering'] = {'status': 'failed', 'error': str(e)}
    
    def execute_genetic_optimization(self):
        """Executa otimiza√ß√£o gen√©tica completa"""
        print("\nüß¨ FASE 5: Otimiza√ß√£o Gen√©tica de Seeds")
        print("-" * 50)
        
        try:
            # Obter candidatos das an√°lises anteriores
            all_candidates = []
            
            if 'quantum' in self.consolidated_results and self.consolidated_results['quantum']['status'] == 'completed':
                all_candidates.extend(self.consolidated_results['quantum'].get('prng_candidates', []))
            
            if 'reverse_engineering' in self.consolidated_results and self.consolidated_results['reverse_engineering']['status'] == 'completed':
                all_candidates.extend(self.consolidated_results['reverse_engineering'].get('prng_candidates', []))
            
            # Inicializar otimizador gen√©tico
            self.genetic_optimizer = GeneticOptimizationEngine(self.historical_data, all_candidates)
            
            # Executar otimiza√ß√£o
            print("üß¨ Executando otimiza√ß√£o gen√©tica...")
            optimization_results = self.genetic_optimizer.optimize_all_candidates()
            
            # Gerar relat√≥rio de otimiza√ß√£o
            optimization_report = self.genetic_optimizer.generate_optimization_report()
            
            # Visualiza√ß√µes
            self.genetic_optimizer.visualize_optimization_results()
            
            # Armazenar resultados
            self.consolidated_results['genetic_optimization'] = {
                'optimization_results': optimization_results,
                'best_solutions': self.genetic_optimizer.best_solutions,
                'report': optimization_report,
                'status': 'completed'
            }
            
            print(f"‚úÖ Otimiza√ß√£o gen√©tica completa - {len(self.genetic_optimizer.best_solutions)} solu√ß√µes encontradas")
            
        except Exception as e:
            print(f"‚ùå Erro na otimiza√ß√£o gen√©tica: {e}")
            self.consolidated_results['genetic_optimization'] = {'status': 'failed', 'error': str(e)}
    
    def consolidate_all_results(self):
        """Consolida todos os resultados em estrutura unificada"""
        print("\nüîó FASE 6: Consolida√ß√£o e Correla√ß√£o de Resultados")
        print("-" * 50)
        
        print("üîÑ Consolidando resultados de todas as an√°lises...")
        
        # Extrair pontos de mudan√ßa de diferentes an√°lises
        all_change_points = self.extract_all_change_points()
        
        # Consolidar candidatos PRNG
        all_prng_candidates = self.extract_all_prng_candidates()
        
        # Consolidar melhores solu√ß√µes
        all_best_solutions = self.extract_all_best_solutions()
        
        # An√°lise de correla√ß√£o cruzada
        cross_correlations = self.analyze_cross_correlations(all_change_points, all_prng_candidates, all_best_solutions)
        
        # Consolidar em estrutura final
        self.consolidated_results['consolidated'] = {
            'all_change_points': all_change_points,
            'all_prng_candidates': all_prng_candidates,
            'all_best_solutions': all_best_solutions,
            'cross_correlations': cross_correlations,
            'execution_timestamp': self.timestamp,
            'data_summary': self.generate_data_summary()
        }
        
        print(f"‚úÖ Consolida√ß√£o completa:")
        print(f"   - {len(all_change_points)} pontos de mudan√ßa totais")
        print(f"   - {len(all_prng_candidates)} candidatos PRNG totais")
        print(f"   - {len(all_best_solutions)} melhores solu√ß√µes totais")
    
    def extract_all_change_points(self):
        """Extrai pontos de mudan√ßa de todas as an√°lises"""
        all_points = []
        
        # Pontos da an√°lise temporal
        if 'temporal' in self.consolidated_results and self.consolidated_results['temporal']['status'] == 'completed':
            temporal_points = self.consolidated_results['temporal'].get('change_points', [])
            for point in temporal_points:
                all_points.append({
                    'position': point,
                    'source': 'temporal_analysis',
                    'confidence': 1.0,  # Assumir alta confian√ßa
                    'method': 'multi_method_consensus'
                })
        
        # Pontos da an√°lise qu√¢ntica (se detectados)
        if 'quantum' in self.consolidated_results and self.consolidated_results['quantum']['status'] == 'completed':
            quantum_results = self.consolidated_results['quantum'].get('analysis_results', {})
            # Extrair pontos de mudan√ßa impl√≠citos da an√°lise qu√¢ntica
            if 'entropy' in quantum_results:
                entropies = quantum_results['entropy'].get('von_neumann_entropies', [])
                if entropies:
                    # Detectar mudan√ßas abruptas na entropia
                    for i in range(1, len(entropies)):
                        entropy_change = abs(entropies[i] - entropies[i-1])
                        if entropy_change > 0.5:  # Threshold para mudan√ßa significativa
                            all_points.append({
                                'position': i,
                                'source': 'quantum_entropy',
                                'confidence': min(1.0, entropy_change),
                                'method': 'entropy_discontinuity'
                            })
        
        # Consolidar pontos pr√≥ximos
        consolidated_points = self.consolidate_nearby_points(all_points)
        
        return consolidated_points
    
    def consolidate_nearby_points(self, points, threshold=30):
        """Consolida pontos pr√≥ximos em um √∫nico ponto"""
        if not points:
            return []
        
        # Ordenar por posi√ß√£o
        sorted_points = sorted(points, key=lambda x: x['position'])
        
        consolidated = []
        current_group = [sorted_points[0]]
        
        for point in sorted_points[1:]:
            if point['position'] - current_group[-1]['position'] <= threshold:
                current_group.append(point)
            else:
                # Processar grupo atual
                consolidated_point = self.merge_point_group(current_group)
                consolidated.append(consolidated_point)
                current_group = [point]
        
        # Processar √∫ltimo grupo
        if current_group:
            consolidated_point = self.merge_point_group(current_group)
            consolidated.append(consolidated_point)
        
        return consolidated
    
    def merge_point_group(self, point_group):
        """Merge um grupo de pontos pr√≥ximos"""
        if len(point_group) == 1:
            return point_group[0]
        
        # Calcular posi√ß√£o m√©dia ponderada por confian√ßa
        total_weight = sum(p['confidence'] for p in point_group)
        weighted_position = sum(p['position'] * p['confidence'] for p in point_group) / total_weight
        
        # Combinar fontes
        sources = list(set(p['source'] for p in point_group))
        methods = list(set(p['method'] for p in point_group))
        
        return {
            'position': int(weighted_position),
            'source': '_'.join(sources),
            'confidence': min(1.0, total_weight / len(point_group)),
            'method': '_'.join(methods),
            'group_size': len(point_group)
        }
    
    def extract_all_prng_candidates(self):
        """Extrai candidatos PRNG de todas as an√°lises"""
        all_candidates = []
        
        # Candidatos da an√°lise qu√¢ntica
        if 'quantum' in self.consolidated_results and self.consolidated_results['quantum']['status'] == 'completed':
            quantum_candidates = self.consolidated_results['quantum'].get('prng_candidates', [])
            for candidate in quantum_candidates:
                candidate['analysis_source'] = 'quantum'
                all_candidates.append(candidate)
        
        # Candidatos da engenharia reversa
        if 'reverse_engineering' in self.consolidated_results and self.consolidated_results['reverse_engineering']['status'] == 'completed':
            reverse_candidates = self.consolidated_results['reverse_engineering'].get('prng_candidates', [])
            for candidate in reverse_candidates:
                candidate['analysis_source'] = 'reverse_engineering'
                all_candidates.append(candidate)
        
        # Ordenar por confian√ßa
        all_candidates.sort(key=lambda x: x.get('confidence', 0), reverse=True)
        
        return all_candidates
    
    def extract_all_best_solutions(self):
        """Extrai melhores solu√ß√µes de todas as otimiza√ß√µes"""
        all_solutions = []
        
        # Solu√ß√µes da otimiza√ß√£o gen√©tica
        if 'genetic_optimization' in self.consolidated_results and self.consolidated_results['genetic_optimization']['status'] == 'completed':
            genetic_solutions = self.consolidated_results['genetic_optimization'].get('best_solutions', [])
            for solution in genetic_solutions:
                solution['optimization_source'] = 'genetic_algorithm'
                all_solutions.append(solution)
        
        # Ordenar por score
        all_solutions.sort(key=lambda x: x.get('score', 0), reverse=True)
        
        return all_solutions
    
    def analyze_cross_correlations(self, all_change_points=None, all_prng_candidates=None, all_best_solutions=None):
        """Analisa correla√ß√µes cruzadas entre diferentes an√°lises"""
        correlations = {}
        
        # Correla√ß√£o entre pontos de mudan√ßa de diferentes fontes
        correlations['change_points'] = self.correlate_change_points(all_change_points)
        
        # Correla√ß√£o entre candidatos PRNG
        correlations['prng_candidates'] = self.correlate_prng_candidates(all_prng_candidates)
        
        # Correla√ß√£o entre solu√ß√µes
        correlations['solutions'] = self.correlate_solutions(all_best_solutions)
        
        return correlations
    
    def correlate_change_points(self, all_points=None):
        """Correlaciona pontos de mudan√ßa entre an√°lises"""
        if all_points is None:
            all_points = self.consolidated_results.get('consolidated', {}).get('all_change_points', [])
        
        # Agrupar por fonte
        sources = {}
        for point in all_points:
            source = point['source']
            if source not in sources:
                sources[source] = []
            sources[source].append(point['position'])
        
        # Calcular correla√ß√µes
        correlations = {}
        source_names = list(sources.keys())
        
        for i, source1 in enumerate(source_names):
            for source2 in source_names[i+1:]:
                correlation = self.calculate_position_correlation(sources[source1], sources[source2])
                correlations[f"{source1}_vs_{source2}"] = correlation
        
        return correlations
    
    def calculate_position_correlation(self, positions1, positions2, tolerance=50):
        """Calcula correla√ß√£o entre listas de posi√ß√µes"""
        if not positions1 or not positions2:
            return 0
        
        matches = 0
        for pos1 in positions1:
            for pos2 in positions2:
                if abs(pos1 - pos2) <= tolerance:
                    matches += 1
                    break
        
        # Jaccard similarity
        total_unique = len(set(positions1) | set(positions2))
        return matches / total_unique if total_unique > 0 else 0
    
    def correlate_prng_candidates(self, all_candidates=None):
        """Correlaciona candidatos PRNG entre an√°lises"""
        if all_candidates is None:
            all_candidates = self.consolidated_results.get('consolidated', {}).get('all_prng_candidates', [])
        
        # Agrupar por fonte
        by_source = {}
        for candidate in all_candidates:
            source = candidate.get('analysis_source', 'unknown')
            if source not in by_source:
                by_source[source] = []
            by_source[source].append(candidate)
        
        # Analisar consenso por tipo PRNG
        type_consensus = {}
        for candidate in all_candidates:
            prng_type = candidate.get('type', 'unknown')
            if prng_type not in type_consensus:
                type_consensus[prng_type] = []
            type_consensus[prng_type].append({
                'source': candidate.get('analysis_source'),
                'confidence': candidate.get('confidence', 0)
            })
        
        return {
            'by_source': {source: len(candidates) for source, candidates in by_source.items()},
            'type_consensus': type_consensus
        }
    
    def correlate_solutions(self, all_solutions=None):
        """Correlaciona solu√ß√µes entre otimiza√ß√µes"""
        if all_solutions is None:
            all_solutions = self.consolidated_results.get('consolidated', {}).get('all_best_solutions', [])
        
        if not all_solutions:
            return {'status': 'no_solutions'}
        
        # Analisar distribui√ß√£o de scores
        scores = [sol.get('score', 0) for sol in all_solutions]
        
        # Analisar consenso de m√©todos
        methods = [sol.get('method', 'unknown') for sol in all_solutions]
        method_counts = {}
        for method in methods:
            method_counts[method] = method_counts.get(method, 0) + 1
        
        return {
            'score_distribution': {
                'mean': np.mean(scores),
                'std': np.std(scores),
                'max': max(scores),
                'min': min(scores)
            },
            'method_consensus': method_counts,
            'total_solutions': len(all_solutions)
        }
    
    def generate_data_summary(self):
        """Gera resumo dos dados analisados"""
        return {
            'total_draws': len(self.historical_data),
            'date_range': {
                'first': self.historical_data[0].get('concurso', 'N/A'),
                'last': self.historical_data[-1].get('concurso', 'N/A')
            },
            'analysis_modules': list(self.consolidated_results.keys()),
            'successful_analyses': len([k for k, v in self.consolidated_results.items() 
                                      if v.get('status') == 'completed']),
            'failed_analyses': len([k for k, v in self.consolidated_results.items() 
                                  if v.get('status') == 'failed'])
        }
    
    def calculate_confidence_scores(self):
        """Calcula scores de confian√ßa para principais descobertas"""
        print("\nüìä FASE 7: C√°lculo de Scores de Confian√ßa")
        print("-" * 50)
        
        # Score de detec√ß√£o de pontos de mudan√ßa
        change_points_confidence = self.calculate_change_points_confidence()
        
        # Score de detec√ß√£o PRNG
        prng_detection_confidence = self.calculate_prng_detection_confidence()
        
        # Score de otimiza√ß√£o
        optimization_confidence = self.calculate_optimization_confidence()
        
        # Score geral
        overall_confidence = self.calculate_overall_confidence(
            change_points_confidence, prng_detection_confidence, optimization_confidence
        )
        
        self.confidence_scores = {
            'change_points': change_points_confidence,
            'prng_detection': prng_detection_confidence,
            'optimization': optimization_confidence,
            'overall': overall_confidence,
            'calculation_timestamp': datetime.now().isoformat()
        }
        
        print(f"‚úÖ Scores de confian√ßa calculados:")
        print(f"   - Pontos de mudan√ßa: {change_points_confidence:.3f}")
        print(f"   - Detec√ß√£o PRNG: {prng_detection_confidence:.3f}")
        print(f"   - Otimiza√ß√£o: {optimization_confidence:.3f}")
        print(f"   - Score geral: {overall_confidence:.3f}")
    
    def calculate_change_points_confidence(self):
        """Calcula confian√ßa na detec√ß√£o de pontos de mudan√ßa"""
        consolidated = self.consolidated_results.get('consolidated', {})
        change_points = consolidated.get('all_change_points', [])
        
        if not change_points:
            return 0
        
        # Fatores de confian√ßa
        confidence_factors = []
        
        # 1. N√∫mero de pontos detectados (mais pontos = menor confian√ßa individual)
        num_points_factor = max(0, 1 - len(change_points) / 50)
        confidence_factors.append(num_points_factor)
        
        # 2. Consenso entre m√©todos
        multi_source_points = [p for p in change_points if p.get('group_size', 1) > 1]
        consensus_factor = len(multi_source_points) / len(change_points)
        confidence_factors.append(consensus_factor)
        
        # 3. Confian√ßa m√©dia dos pontos
        avg_confidence = np.mean([p.get('confidence', 0) for p in change_points])
        confidence_factors.append(avg_confidence)
        
        return np.mean(confidence_factors)
    
    def calculate_prng_detection_confidence(self):
        """Calcula confian√ßa na detec√ß√£o de PRNG"""
        consolidated = self.consolidated_results.get('consolidated', {})
        prng_candidates = consolidated.get('all_prng_candidates', [])
        
        if not prng_candidates:
            return 0
        
        # Fatores de confian√ßa
        confidence_factors = []
        
        # 1. Melhor confian√ßa individual
        best_confidence = max([c.get('confidence', 0) for c in prng_candidates])
        confidence_factors.append(best_confidence)
        
        # 2. Consenso entre an√°lises
        correlations = consolidated.get('cross_correlations', {}).get('prng_candidates', {})
        type_consensus = correlations.get('type_consensus', {})
        
        if type_consensus:
            # Calcular consenso baseado em tipos detectados por m√∫ltiplas fontes
            multi_source_types = [t for t, sources in type_consensus.items() if len(sources) > 1]
            consensus_factor = len(multi_source_types) / len(type_consensus)
            confidence_factors.append(consensus_factor)
        else:
            confidence_factors.append(0)
        
        # 3. Diversidade de m√©todos
        sources = set(c.get('analysis_source', 'unknown') for c in prng_candidates)
        diversity_factor = min(1.0, len(sources) / 2)  # Esperamos pelo menos 2 fontes
        confidence_factors.append(diversity_factor)
        
        return np.mean(confidence_factors)
    
    def calculate_optimization_confidence(self):
        """Calcula confian√ßa na otimiza√ß√£o"""
        consolidated = self.consolidated_results.get('consolidated', {})
        solutions = consolidated.get('all_best_solutions', [])
        
        if not solutions:
            return 0
        
        # Fatores de confian√ßa
        confidence_factors = []
        
        # 1. Melhor score obtido
        best_score = max([s.get('score', 0) for s in solutions])
        confidence_factors.append(best_score)
        
        # 2. Consist√™ncia entre solu√ß√µes
        correlations = consolidated.get('cross_correlations', {}).get('solutions', {})
        score_dist = correlations.get('score_distribution', {})
        
        if score_dist and score_dist.get('mean', 0) > 0:
            # Consist√™ncia baseada em coeficiente de varia√ß√£o
            cv = score_dist.get('std', 0) / score_dist.get('mean', 1)
            consistency_factor = max(0, 1 - cv)
            confidence_factors.append(consistency_factor)
        else:
            confidence_factors.append(0)
        
        # 3. N√∫mero de solu√ß√µes encontradas
        num_solutions_factor = min(1.0, len(solutions) / 10)  # Normalizar por 10 solu√ß√µes
        confidence_factors.append(num_solutions_factor)
        
        return np.mean(confidence_factors)
    
    def calculate_overall_confidence(self, change_points_conf, prng_conf, optimization_conf):
        """Calcula confian√ßa geral"""
        # Pesos para diferentes aspectos
        weights = {
            'change_points': 0.3,
            'prng_detection': 0.4,
            'optimization': 0.3
        }
        
        overall = (
            weights['change_points'] * change_points_conf +
            weights['prng_detection'] * prng_conf +
            weights['optimization'] * optimization_conf
        )
        
        return overall
    
    def generate_master_report(self):
        """Gera relat√≥rio mestre final"""
        print("\nüìã FASE 8: Gera√ß√£o de Relat√≥rio Mestre")
        print("-" * 50)
        
        print("üìù Compilando relat√≥rio abrangente...")
        
        self.master_report = {
            'metadata': {
                'title': 'Relat√≥rio Mestre - An√°lise Abrangente Mega Sena',
                'subtitle': 'Descoberta de Seeds e An√°lise de PRNGs',
                'generated_at': datetime.now().isoformat(),
                'execution_id': self.timestamp,
                'total_execution_time': 'N/A',  # Calcular se necess√°rio
                'version': '1.0.0'
            },
            
            'executive_summary': self.generate_executive_summary(),
            
            'methodology': {
                'description': 'An√°lise multi-camada utilizando t√©cnicas de computa√ß√£o qu√¢ntica, engenharia reversa, otimiza√ß√£o gen√©tica e an√°lise temporal',
                'modules_used': [
                    'Detector Temporal Avan√ßado',
                    'Analisador Qu√¢ntico de PRNGs',
                    'Engenheiro Reverso Multi-PRNG',
                    'Motor de Otimiza√ß√£o Gen√©tica'
                ],
                'data_analyzed': self.consolidated_results['consolidated']['data_summary']
            },
            
            'detailed_findings': {
                'temporal_analysis': self.summarize_temporal_findings(),
                'quantum_analysis': self.summarize_quantum_findings(),
                'reverse_engineering': self.summarize_reverse_engineering_findings(),
                'genetic_optimization': self.summarize_optimization_findings()
            },
            
            'consolidated_results': self.consolidated_results['consolidated'],
            
            'confidence_assessment': {
                'scores': self.confidence_scores,
                'reliability_analysis': self.generate_reliability_analysis(),
                'validation_requirements': self.generate_validation_requirements()
            },
            
            'practical_implications': self.generate_practical_implications(),
            
            'technical_appendices': self.generate_technical_appendices(),
            
            'conclusions_and_recommendations': self.generate_conclusions_and_recommendations()
        }
        
        # Salvar relat√≥rio
        report_path = self.output_dir / f"master_report_{self.timestamp}.json"
        with open(report_path, 'w') as f:
            json.dump(self.master_report, f, indent=2, default=str)
        
        # Gerar vers√£o em texto
        self.generate_text_report()
        
        print(f"‚úÖ Relat√≥rio mestre gerado: {report_path}")
    
    def generate_executive_summary(self):
        """Gera sum√°rio executivo"""
        consolidated = self.consolidated_results.get('consolidated', {})
        
        # Principais descobertas
        key_findings = []
        
        # Pontos de mudan√ßa
        change_points = consolidated.get('all_change_points', [])
        if change_points:
            key_findings.append(f"{len(change_points)} pontos de mudan√ßa de algoritmo detectados")
        
        # Candidatos PRNG
        prng_candidates = consolidated.get('all_prng_candidates', [])
        if prng_candidates:
            best_prng = prng_candidates[0]
            key_findings.append(f"Algoritmo mais prov√°vel: {best_prng.get('type', 'desconhecido')} (confian√ßa: {best_prng.get('confidence', 0):.3f})")
        
        # Melhores solu√ß√µes
        solutions = consolidated.get('all_best_solutions', [])
        if solutions:
            best_solution = solutions[0]
            key_findings.append(f"Melhor solu√ß√£o de par√¢metros: {best_solution.get('method', 'desconhecido')} (score: {best_solution.get('score', 0):.6f})")
        
        # Confian√ßa geral
        overall_confidence = self.confidence_scores.get('overall', 0)
        confidence_level = 'alta' if overall_confidence > 0.7 else 'm√©dia' if overall_confidence > 0.4 else 'baixa'
        
        return {
            'analysis_scope': f"An√°lise abrangente de {len(self.historical_data)} sorteios da Mega Sena",
            'key_findings': key_findings,
            'confidence_level': f"Confian√ßa geral: {confidence_level} ({overall_confidence:.3f})",
            'main_conclusion': self.generate_main_conclusion(),
            'actionable_insights': self.generate_actionable_insights()
        }
    
    def generate_main_conclusion(self):
        """Gera conclus√£o principal"""
        overall_confidence = self.confidence_scores.get('overall', 0)
        
        if overall_confidence > 0.7:
            return "Sistema da Mega Sena apresenta caracter√≠sticas determin√≠sticas detect√°veis com alta confian√ßa."
        elif overall_confidence > 0.4:
            return "Sistema da Mega Sena apresenta padr√µes parcialmente detect√°veis, requerendo valida√ß√£o adicional."
        else:
            return "Sistema da Mega Sena apresenta caracter√≠sticas predominantemente aleat√≥rias ou utiliza algoritmos complexos n√£o detectados."
    
    def generate_actionable_insights(self):
        """Gera insights acion√°veis"""
        insights = []
        
        consolidated = self.consolidated_results.get('consolidated', {})
        
        # Insights sobre pontos de mudan√ßa
        change_points = consolidated.get('all_change_points', [])
        if change_points:
            insights.append(f"Monitorar sorteios pr√≥ximos √†s posi√ß√µes {[p['position'] for p in change_points[:3]]} para valida√ß√£o")
        
        # Insights sobre PRNG
        prng_candidates = consolidated.get('all_prng_candidates', [])
        if prng_candidates and prng_candidates[0].get('confidence', 0) > 0.5:
            best_prng = prng_candidates[0]
            insights.append(f"Focar an√°lise detalhada no algoritmo {best_prng['type']}")
        
        # Insights sobre solu√ß√µes
        solutions = consolidated.get('all_best_solutions', [])
        if solutions and solutions[0].get('score', 0) > 0.5:
            insights.append("Validar par√¢metros encontrados com sorteios futuros")
        
        if not insights:
            insights.append("Realizar an√°lise adicional com m√©todos alternativos")
        
        return insights
    
    def summarize_temporal_findings(self):
        """Sumariza descobertas da an√°lise temporal"""
        temporal_results = self.consolidated_results.get('temporal', {})
        
        if temporal_results.get('status') != 'completed':
            return {'status': 'failed', 'error': temporal_results.get('error')}
        
        change_points = temporal_results.get('change_points', [])
        regimes = temporal_results.get('regimes', [])
        
        return {
            'change_points_detected': len(change_points),
            'regime_count': len(regimes),
            'average_regime_duration': np.mean([r['duration'] for r in regimes]) if regimes else 0,
            'most_significant_changes': change_points[:5] if change_points else [],
            'temporal_patterns': self.analyze_temporal_patterns(regimes)
        }
    
    def analyze_temporal_patterns(self, regimes):
        """Analisa padr√µes temporais nos regimes"""
        if not regimes:
            return {}
        
        # Analisar correla√ß√µes entre regimes
        correlations = [r.get('correlation_pattern', 0) for r in regimes]
        entropies = [r.get('entropy', 0) for r in regimes]
        
        return {
            'correlation_trend': 'increasing' if correlations[-1] > correlations[0] else 'decreasing' if len(correlations) > 1 else 'stable',
            'entropy_trend': 'increasing' if entropies[-1] > entropies[0] else 'decreasing' if len(entropies) > 1 else 'stable',
            'regime_stability': np.std([r['duration'] for r in regimes]) / np.mean([r['duration'] for r in regimes]) if regimes else 0
        }
    
    def summarize_quantum_findings(self):
        """Sumariza descobertas da an√°lise qu√¢ntica"""
        quantum_results = self.consolidated_results.get('quantum', {})
        
        if quantum_results.get('status') != 'completed':
            return {'status': 'failed', 'error': quantum_results.get('error')}
        
        prng_candidates = quantum_results.get('prng_candidates', [])
        analysis_results = quantum_results.get('analysis_results', {})
        
        # Sumarizar descobertas qu√¢nticas
        quantum_summary = {}
        
        if 'entropy' in analysis_results:
            entropy_data = analysis_results['entropy']
            quantum_summary['entropy_analysis'] = {
                'mean_von_neumann': entropy_data.get('mean_entropy', 0),
                'entropy_variance': entropy_data.get('entropy_variance', 0),
                'mean_mutual_information': np.mean(entropy_data.get('mutual_informations', [0]))
            }
        
        if 'entanglement' in analysis_results:
            entanglement_data = analysis_results['entanglement']
            quantum_summary['entanglement_analysis'] = {
                'mean_entanglement': np.mean([e.get('negativity', 0) for e in entanglement_data]),
                'entanglement_variability': np.std([e.get('negativity', 0) for e in entanglement_data])
            }
        
        return {
            'prng_candidates_detected': len(prng_candidates),
            'best_quantum_candidate': prng_candidates[0] if prng_candidates else None,
            'quantum_metrics': quantum_summary,
            'quantum_complexity_score': self.calculate_quantum_complexity_score(analysis_results)
        }
    
    def calculate_quantum_complexity_score(self, analysis_results):
        """Calcula score de complexidade qu√¢ntica"""
        complexity_factors = []
        
        # Complexidade baseada na entropia
        if 'entropy' in analysis_results:
            entropy_variance = analysis_results['entropy'].get('entropy_variance', 0)
            complexity_factors.append(min(1.0, entropy_variance * 10))
        
        # Complexidade baseada no entrela√ßamento
        if 'entanglement' in analysis_results:
            entanglement_data = analysis_results['entanglement']
            mean_entanglement = np.mean([e.get('negativity', 0) for e in entanglement_data])
            complexity_factors.append(mean_entanglement)
        
        # Complexidade baseada na coer√™ncia
        if 'coherence' in analysis_results:
            coherence_data = analysis_results['coherence']
            mean_coherence = np.mean([c['coherence'].get('l1_coherence', 0) for c in coherence_data])
            complexity_factors.append(mean_coherence)
        
        return np.mean(complexity_factors) if complexity_factors else 0
    
    def summarize_reverse_engineering_findings(self):
        """Sumariza descobertas da engenharia reversa"""
        reverse_results = self.consolidated_results.get('reverse_engineering', {})
        
        if reverse_results.get('status') != 'completed':
            return {'status': 'failed', 'error': reverse_results.get('error')}
        
        prng_candidates = reverse_results.get('prng_candidates', [])
        analysis_results = reverse_results.get('analysis_results', {})
        
        # Analisar resultados por tipo de PRNG
        prng_analysis = {}
        for prng_type, result in analysis_results.items():
            if isinstance(result, dict) and result.get('best_match'):
                prng_analysis[prng_type] = {
                    'confidence': result['best_match'].get('confidence', 0),
                    'parameters': result['best_match']
                }
        
        return {
            'total_prng_types_analyzed': len(analysis_results),
            'successful_detections': len([r for r in analysis_results.values() 
                                        if isinstance(r, dict) and r.get('best_match')]),
            'prng_analysis_by_type': prng_analysis,
            'best_reverse_candidate': prng_candidates[0] if prng_candidates else None,
            'reverse_engineering_quality': self.assess_reverse_engineering_quality(analysis_results)
        }
    
    def assess_reverse_engineering_quality(self, analysis_results):
        """Avalia qualidade da engenharia reversa"""
        quality_factors = []
        
        # N√∫mero de tipos PRNG com detec√ß√£o bem-sucedida
        successful_detections = len([r for r in analysis_results.values() 
                                   if isinstance(r, dict) and r.get('best_match')])
        detection_rate = successful_detections / len(analysis_results) if analysis_results else 0
        quality_factors.append(detection_rate)
        
        # Confian√ßa m√©dia das detec√ß√µes
        confidences = []
        for result in analysis_results.values():
            if isinstance(result, dict) and result.get('best_match'):
                confidences.append(result['best_match'].get('confidence', 0))
        
        avg_confidence = np.mean(confidences) if confidences else 0
        quality_factors.append(avg_confidence)
        
        return np.mean(quality_factors)
    
    def summarize_optimization_findings(self):
        """Sumariza descobertas da otimiza√ß√£o gen√©tica"""
        optimization_results = self.consolidated_results.get('genetic_optimization', {})
        
        if optimization_results.get('status') != 'completed':
            return {'status': 'failed', 'error': optimization_results.get('error')}
        
        best_solutions = optimization_results.get('best_solutions', [])
        opt_results = optimization_results.get('optimization_results', {})
        
        return {
            'total_solutions_found': len(best_solutions),
            'best_solution': best_solutions[0] if best_solutions else None,
            'optimization_methods_used': list(opt_results.keys()),
            'score_distribution': self.analyze_solution_scores(best_solutions),
            'parameter_consensus': self.analyze_parameter_consensus(best_solutions)
        }
    
    def analyze_solution_scores(self, solutions):
        """Analisa distribui√ß√£o de scores das solu√ß√µes"""
        if not solutions:
            return {}
        
        scores = [s.get('score', 0) for s in solutions]
        
        return {
            'mean': np.mean(scores),
            'std': np.std(scores),
            'max': max(scores),
            'min': min(scores),
            'q75': np.percentile(scores, 75),
            'q25': np.percentile(scores, 25)
        }
    
    def analyze_parameter_consensus(self, solutions):
        """Analisa consenso de par√¢metros entre solu√ß√µes"""
        if not solutions:
            return {}
        
        # Coletar par√¢metros mais comuns
        all_methods = [s.get('method', 'unknown') for s in solutions]
        method_counts = {}
        for method in all_methods:
            method_counts[method] = method_counts.get(method, 0) + 1
        
        most_common_method = max(method_counts, key=method_counts.get) if method_counts else None
        
        return {
            'method_distribution': method_counts,
            'most_common_method': most_common_method,
            'method_consensus_rate': max(method_counts.values()) / len(solutions) if method_counts else 0
        }
    
    def generate_reliability_analysis(self):
        """Gera an√°lise de confiabilidade"""
        return {
            'data_quality': {
                'completeness': 1.0,  # Assumindo dados completos
                'consistency': self.assess_data_consistency(),
                'temporal_coverage': f"{len(self.historical_data)} sorteios"
            },
            'method_robustness': {
                'temporal_analysis': 'alta - m√∫ltiplos m√©todos convergem',
                'quantum_analysis': 'm√©dia - dependente de bibliotecas especializadas',
                'reverse_engineering': 'alta - testa m√∫ltiplos algoritmos conhecidos',
                'genetic_optimization': 'alta - algoritmo evolutivo robusto'
            },
            'validation_status': {
                'cross_validation': self.assess_cross_validation(),
                'independent_validation': 'requerida - usar sorteios futuros',
                'statistical_significance': self.assess_statistical_significance()
            }
        }
    
    def assess_data_consistency(self):
        """Avalia consist√™ncia dos dados"""
        # Verificar consist√™ncia b√°sica dos sorteios
        inconsistencies = 0
        
        for draw in self.historical_data:
            numbers = draw.get('numbers', [])
            
            # Verificar se tem 6 n√∫meros
            if len(numbers) != 6:
                inconsistencies += 1
                continue
            
            # Verificar se n√∫meros est√£o no range correto
            if any(n < 1 or n > 60 for n in numbers):
                inconsistencies += 1
                continue
            
            # Verificar se n√£o h√° duplicatas
            if len(set(numbers)) != 6:
                inconsistencies += 1
        
        consistency_rate = 1 - (inconsistencies / len(self.historical_data))
        return f"{consistency_rate:.3f} ({inconsistencies} inconsist√™ncias detectadas)"
    
    def assess_cross_validation(self):
        """Avalia valida√ß√£o cruzada entre m√©todos"""
        consolidated = self.consolidated_results.get('consolidated', {})
        correlations = consolidated.get('cross_correlations', {})
        
        # Verificar correla√ß√µes entre m√©todos
        correlation_quality = []
        
        for correlation_type, data in correlations.items():
            if isinstance(data, dict) and 'consensus_factor' in data:
                correlation_quality.append(data['consensus_factor'])
        
        if correlation_quality:
            avg_correlation = np.mean(correlation_quality)
            return f"boa - correla√ß√£o m√©dia entre m√©todos: {avg_correlation:.3f}"
        else:
            return "limitada - correla√ß√µes n√£o quantificadas"
    
    def assess_statistical_significance(self):
        """Avalia signific√¢ncia estat√≠stica"""
        # Baseado no tamanho da amostra e resultados
        sample_size = len(self.historical_data)
        
        if sample_size > 1000:
            significance = "alta"
        elif sample_size > 500:
            significance = "m√©dia"
        else:
            significance = "baixa"
        
        return f"{significance} - {sample_size} amostras analisadas"
    
    def generate_validation_requirements(self):
        """Gera requisitos de valida√ß√£o"""
        return {
            'immediate_validation': [
                "Verificar predi√ß√µes com pr√≥ximos 10 sorteios",
                "Validar pontos de mudan√ßa identificados",
                "Confirmar par√¢metros PRNG detectados"
            ],
            'long_term_validation': [
                "Monitorar performance por 6 meses",
                "Atualizar an√°lise com novos dados",
                "Refinar modelos baseado em novos padr√µes"
            ],
            'validation_metrics': [
                "Taxa de acerto em predi√ß√µes",
                "Estabilidade de par√¢metros ao longo do tempo",
                "Manuten√ß√£o de padr√µes detectados"
            ]
        }
    
    def generate_practical_implications(self):
        """Gera implica√ß√µes pr√°ticas"""
        consolidated = self.consolidated_results.get('consolidated', {})
        overall_confidence = self.confidence_scores.get('overall', 0)
        
        implications = {
            'system_understanding': self.generate_system_understanding_implications(),
            'predictive_capability': self.generate_predictive_capability_implications(),
            'security_considerations': self.generate_security_implications(),
            'research_directions': self.generate_research_directions()
        }
        
        return implications
    
    def generate_system_understanding_implications(self):
        """Gera implica√ß√µes para entendimento do sistema"""
        change_points = self.consolidated_results.get('consolidated', {}).get('all_change_points', [])
        
        if change_points:
            return [
                f"Sistema apresenta {len(change_points)} mudan√ßas algor√≠tmicas detect√°veis",
                "Comportamento n√£o √© puramente aleat√≥rio",
                "Existe estrutura temporal detect√°vel nos dados"
            ]
        else:
            return [
                "Sistema aparenta ter comportamento consistente ao longo do tempo",
                "N√£o foram detectadas mudan√ßas algor√≠tmicas significativas",
                "Comportamento pr√≥ximo ao verdadeiramente aleat√≥rio"
            ]
    
    def generate_predictive_capability_implications(self):
        """Gera implica√ß√µes para capacidade preditiva"""
        best_solutions = self.consolidated_results.get('consolidated', {}).get('all_best_solutions', [])
        
        if best_solutions and best_solutions[0].get('score', 0) > 0.5:
            return [
                "Detectada capacidade preditiva limitada",
                "Par√¢metros identificados podem permitir predi√ß√µes parciais",
                "Requer valida√ß√£o com dados independentes"
            ]
        else:
            return [
                "Capacidade preditiva n√£o estabelecida com confian√ßa",
                "Sistema apresenta alta entropia",
                "Predi√ß√µes precisas improv√°veis com m√©todos atuais"
            ]
    
    def generate_security_implications(self):
        """Gera implica√ß√µes de seguran√ßa"""
        prng_candidates = self.consolidated_results.get('consolidated', {}).get('all_prng_candidates', [])
        
        if prng_candidates and prng_candidates[0].get('confidence', 0) > 0.7:
            return [
                "Sistema pode ser baseado em PRNG determin√≠stico",
                "Poss√≠vel vulnerabilidade se par√¢metros forem confirmados",
                "Recomenda-se auditoria de seguran√ßa do sistema gerador"
            ]
        else:
            return [
                "N√£o foram identificadas vulnerabilidades √≥bvias",
                "Sistema aparenta usar gera√ß√£o segura",
                "An√°lise adicional recomendada para confirma√ß√£o"
            ]
    
    def generate_research_directions(self):
        """Gera dire√ß√µes de pesquisa"""
        return [
            "Investigar algoritmos criptogr√°ficos avan√ßados",
            "Analisar correla√ß√µes com fatores externos",
            "Desenvolver m√©todos de detec√ß√£o mais sens√≠veis",
            "Estudar padr√µes em outras loterias similares",
            "Aplicar t√©cnicas de deep learning",
            "Investigar poss√≠vel influ√™ncia de hardware espec√≠fico"
        ]
    
    def generate_technical_appendices(self):
        """Gera ap√™ndices t√©cnicos"""
        return {
            'algorithm_details': {
                'temporal_detection': 'M√∫ltiplos m√©todos incluindo clustering, PCA, detec√ß√£o de outliers',
                'quantum_analysis': 'Entropia von Neumann, transformada de Fourier qu√¢ntica, an√°lise de entrela√ßamento',
                'reverse_engineering': 'Teste sistem√°tico de LCG, LFSR, Mersenne Twister, Xorshift, PCG',
                'genetic_optimization': 'Algoritmo gen√©tico com sele√ß√£o por torneio, crossover e muta√ß√£o'
            },
            'parameter_ranges': self.extract_parameter_ranges(),
            'computational_complexity': self.assess_computational_complexity(),
            'library_versions': self.get_library_versions()
        }
    
    def extract_parameter_ranges(self):
        """Extrai ranges de par√¢metros testados"""
        return {
            'LCG_parameters': {
                'a_range': '1 to 2^16',
                'c_range': '0 to 2^16', 
                'm_values': ['2^31-1', '2^32', '2^31']
            },
            'genetic_algorithm': {
                'population_size': 100,
                'generations': 500,
                'mutation_rate': 0.1,
                'crossover_rate': 0.8
            },
            'analysis_windows': {
                'temporal_window': '50-100 sorteios',
                'quantum_analysis': 'sequ√™ncia completa',
                'validation_period': '10-20 sorteios'
            }
        }
    
    def assess_computational_complexity(self):
        """Avalia complexidade computacional"""
        return {
            'temporal_analysis': 'O(n¬≤) para correla√ß√µes, O(n log n) para FFT',
            'quantum_analysis': 'O(n) para entropia, O(n¬≤) para entrela√ßamento',
            'reverse_engineering': 'O(k √ó n) onde k √© n√∫mero de par√¢metros testados',
            'genetic_optimization': 'O(g √ó p √ó n) onde g=gera√ß√µes, p=popula√ß√£o, n=tamanho dados'
        }
    
    def get_library_versions(self):
        """Obt√©m vers√µes das bibliotecas utilizadas"""
        try:
            import sys
            versions = {
                'python': sys.version,
                'numpy': np.__version__,
                'pandas': pd.__version__
            }
            
            try:
                import scipy
                versions['scipy'] = scipy.__version__
            except:
                pass
                
            try:
                import matplotlib
                versions['matplotlib'] = matplotlib.__version__
            except:
                pass
                
            return versions
        except:
            return {'note': 'vers√µes n√£o dispon√≠veis'}
    
    def generate_conclusions_and_recommendations(self):
        """Gera conclus√µes e recomenda√ß√µes finais"""
        overall_confidence = self.confidence_scores.get('overall', 0)
        
        conclusions = {
            'primary_conclusions': self.generate_primary_conclusions(),
            'confidence_assessment': self.generate_confidence_assessment(),
            'limitations': self.identify_limitations(),
            'recommendations': {
                'immediate_actions': self.generate_immediate_recommendations(),
                'future_research': self.generate_future_research_recommendations(),
                'validation_strategy': self.generate_validation_strategy()
            }
        }
        
        return conclusions
    
    def generate_primary_conclusions(self):
        """Gera conclus√µes prim√°rias"""
        consolidated = self.consolidated_results.get('consolidated', {})
        change_points = consolidated.get('all_change_points', [])
        prng_candidates = consolidated.get('all_prng_candidates', [])
        solutions = consolidated.get('all_best_solutions', [])
        
        conclusions = []
        
        # Conclus√£o sobre determinismo
        if change_points and prng_candidates:
            conclusions.append("Sistema apresenta caracter√≠sticas determin√≠sticas detect√°veis")
        else:
            conclusions.append("Sistema aparenta comportamento predominantemente aleat√≥rio")
        
        # Conclus√£o sobre mudan√ßas temporais
        if len(change_points) > 3:
            conclusions.append("M√∫ltiplas mudan√ßas algor√≠tmicas detectadas ao longo do tempo")
        elif change_points:
            conclusions.append("Poucas mudan√ßas algor√≠tmicas detectadas")
        else:
            conclusions.append("Comportamento algor√≠tmico est√°vel ao longo do tempo")
        
        # Conclus√£o sobre predi√ß√£o
        if solutions and solutions[0].get('score', 0) > 0.5:
            conclusions.append("Capacidade preditiva limitada demonstrada")
        else:
            conclusions.append("Capacidade preditiva n√£o estabelecida")
        
        return conclusions
    
    def generate_confidence_assessment(self):
        """Gera avalia√ß√£o de confian√ßa"""
        overall_confidence = self.confidence_scores.get('overall', 0)
        
        if overall_confidence > 0.7:
            assessment = "Alta confian√ßa nos resultados obtidos"
        elif overall_confidence > 0.4:
            assessment = "Confian√ßa moderada - resultados promissores mas requerem valida√ß√£o"
        else:
            assessment = "Baixa confian√ßa - resultados inconclusivos"
        
        return {
            'overall_assessment': assessment,
            'confidence_score': overall_confidence,
            'key_uncertainties': self.identify_key_uncertainties()
        }
    
    def identify_key_uncertainties(self):
        """Identifica principais incertezas"""
        uncertainties = []
        
        # Incertezas baseadas nos resultados
        if self.confidence_scores.get('prng_detection', 0) < 0.5:
            uncertainties.append("Tipo de algoritmo PRNG n√£o definitivamente identificado")
        
        if self.confidence_scores.get('optimization', 0) < 0.5:
            uncertainties.append("Par√¢metros √≥timos n√£o convergidos com alta confian√ßa")
        
        if len(self.consolidated_results.get('consolidated', {}).get('all_change_points', [])) == 0:
            uncertainties.append("Aus√™ncia de pontos de mudan√ßa pode indicar sistema mais complexo")
        
        return uncertainties
    
    def identify_limitations(self):
        """Identifica limita√ß√µes do estudo"""
        return [
            "An√°lise baseada apenas em n√∫meros sorteados, sem acesso ao c√≥digo fonte",
            "Pressuposi√ß√µes sobre tipos de PRNG podem estar incompletas",
            "Valida√ß√£o requer dados futuros n√£o dispon√≠veis no momento da an√°lise",
            "Poss√≠vel influ√™ncia de fatores externos n√£o considerados",
            "Limita√ß√µes computacionais restringem espa√ßo de busca exaustiva"
        ]
    
    def generate_immediate_recommendations(self):
        """Gera recomenda√ß√µes imediatas"""
        recommendations = []
        
        consolidated = self.consolidated_results.get('consolidated', {})
        
        # Recomenda√ß√µes baseadas em descobertas
        if consolidated.get('all_change_points'):
            recommendations.append("Validar pontos de mudan√ßa com an√°lise independente")
        
        if consolidated.get('all_prng_candidates'):
            best_prng = consolidated['all_prng_candidates'][0]
            if best_prng.get('confidence', 0) > 0.5:
                recommendations.append(f"Investigar em detalhes o algoritmo {best_prng['type']}")
        
        if consolidated.get('all_best_solutions'):
            recommendations.append("Testar par√¢metros encontrados com sorteios futuros")
        
        recommendations.append("Documentar metodologia para reprodutibilidade")
        recommendations.append("Compartilhar resultados com comunidade de pesquisa")
        
        return recommendations
    
    def generate_future_research_recommendations(self):
        """Gera recomenda√ß√µes para pesquisa futura"""
        return [
            "Aplicar an√°lise similar a outras loterias nacionais e internacionais",
            "Desenvolver m√©todos de detec√ß√£o baseados em deep learning",
            "Investigar correla√ß√µes com dados meteorol√≥gicos e outros fatores externos",
            "Estudar poss√≠vel influ√™ncia de hardware espec√≠fico nos geradores",
            "Desenvolver testes estat√≠sticos mais sens√≠veis para detec√ß√£o de padr√µes",
            "Colaborar com especialistas em criptografia para an√°lise de seguran√ßa"
        ]
    
    def generate_validation_strategy(self):
        """Gera estrat√©gia de valida√ß√£o"""
        return {
            'short_term': [
                "Coletar pr√≥ximos 20 sorteios para valida√ß√£o imediata",
                "Aplicar modelos encontrados para predi√ß√£o",
                "Medir taxa de acerto e comparar com baseline aleat√≥rio"
            ],
            'medium_term': [
                "Monitorar performance por 6 meses",
                "Refinar par√¢metros baseado em novos dados",
                "Desenvolver m√©tricas de performance espec√≠ficas"
            ],
            'long_term': [
                "Estabelecer protocolo de monitoramento cont√≠nuo",
                "Criar sistema de alerta para mudan√ßas detectadas",
                "Desenvolver framework para an√°lise automatizada"
            ]
        }
    
    def generate_text_report(self):
        """Gera vers√£o em texto do relat√≥rio"""
        print("üìÑ Gerando relat√≥rio em texto...")
        
        text_report = f"""
# RELAT√ìRIO MESTRE - AN√ÅLISE ABRANGENTE MEGA SENA
## Descoberta de Seeds e An√°lise de PRNGs

**Gerado em:** {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}
**ID de Execu√ß√£o:** {self.timestamp}

---

## SUM√ÅRIO EXECUTIVO

{self.master_report['executive_summary']['analysis_scope']}

### Principais Descobertas:
"""
        
        for finding in self.master_report['executive_summary']['key_findings']:
            text_report += f"‚Ä¢ {finding}\n"
        
        text_report += f"""
### {self.master_report['executive_summary']['confidence_level']}

### Conclus√£o Principal:
{self.master_report['executive_summary']['main_conclusion']}

### Insights Acion√°veis:
"""
        
        for insight in self.master_report['executive_summary']['actionable_insights']:
            text_report += f"‚Ä¢ {insight}\n"
        
        text_report += f"""

---

## METODOLOGIA

{self.master_report['methodology']['description']}

### M√≥dulos Utilizados:
"""
        
        for module in self.master_report['methodology']['modules_used']:
            text_report += f"‚Ä¢ {module}\n"
        
        text_report += f"""

### Dados Analisados:
‚Ä¢ Total de sorteios: {self.master_report['methodology']['data_analyzed']['total_draws']}
‚Ä¢ Per√≠odo: Concurso {self.master_report['methodology']['data_analyzed']['date_range']['first']} ao {self.master_report['methodology']['data_analyzed']['date_range']['last']}
‚Ä¢ An√°lises bem-sucedidas: {self.master_report['methodology']['data_analyzed']['successful_analyses']}

---

## PRINCIPAIS RESULTADOS

### An√°lise Temporal
"""
        
        temporal = self.master_report['detailed_findings']['temporal_analysis']
        if temporal.get('status') == 'failed':
            text_report += f"‚ùå Falha: {temporal.get('error', 'Erro desconhecido')}\n"
        else:
            text_report += f"‚Ä¢ Pontos de mudan√ßa detectados: {temporal.get('change_points_detected', 0)}\n"
            text_report += f"‚Ä¢ Regimes identificados: {temporal.get('regime_count', 0)}\n"
            text_report += f"‚Ä¢ Dura√ß√£o m√©dia de regime: {temporal.get('average_regime_duration', 0):.1f} sorteios\n"
        
        text_report += """

### An√°lise Qu√¢ntica
"""
        
        quantum = self.master_report['detailed_findings']['quantum_analysis']
        if quantum.get('status') == 'failed':
            text_report += f"‚ùå Falha: {quantum.get('error', 'Erro desconhecido')}\n"
        else:
            text_report += f"‚Ä¢ Candidatos PRNG detectados: {quantum.get('prng_candidates_detected', 0)}\n"
            if quantum.get('best_quantum_candidate'):
                best = quantum['best_quantum_candidate']
                text_report += f"‚Ä¢ Melhor candidato: {best.get('type', 'N/A')} (confian√ßa: {best.get('confidence', 0):.3f})\n"
            text_report += f"‚Ä¢ Score de complexidade qu√¢ntica: {quantum.get('quantum_complexity_score', 0):.3f}\n"
        
        text_report += """

### Engenharia Reversa
"""
        
        reverse = self.master_report['detailed_findings']['reverse_engineering']
        if reverse.get('status') == 'failed':
            text_report += f"‚ùå Falha: {reverse.get('error', 'Erro desconhecido')}\n"
        else:
            text_report += f"‚Ä¢ Tipos PRNG analisados: {reverse.get('total_prng_types_analyzed', 0)}\n"
            text_report += f"‚Ä¢ Detec√ß√µes bem-sucedidas: {reverse.get('successful_detections', 0)}\n"
            text_report += f"‚Ä¢ Qualidade da engenharia reversa: {reverse.get('reverse_engineering_quality', 0):.3f}\n"
        
        text_report += """

### Otimiza√ß√£o Gen√©tica
"""
        
        optimization = self.master_report['detailed_findings']['genetic_optimization']
        if optimization.get('status') == 'failed':
            text_report += f"‚ùå Falha: {optimization.get('error', 'Erro desconhecido')}\n"
        else:
            text_report += f"‚Ä¢ Solu√ß√µes encontradas: {optimization.get('total_solutions_found', 0)}\n"
            if optimization.get('best_solution'):
                best = optimization['best_solution']
                text_report += f"‚Ä¢ Melhor solu√ß√£o: {best.get('method', 'N/A')} (score: {best.get('score', 0):.6f})\n"
        
        text_report += f"""

---

## AVALIA√á√ÉO DE CONFIAN√áA

### Scores de Confian√ßa:
‚Ä¢ Pontos de mudan√ßa: {self.confidence_scores.get('change_points', 0):.3f}
‚Ä¢ Detec√ß√£o PRNG: {self.confidence_scores.get('prng_detection', 0):.3f}
‚Ä¢ Otimiza√ß√£o: {self.confidence_scores.get('optimization', 0):.3f}
‚Ä¢ **Score Geral: {self.confidence_scores.get('overall', 0):.3f}**

### Avalia√ß√£o:
{self.master_report['confidence_assessment']['reliability_analysis']['method_robustness']}

---

## CONCLUS√ïES E RECOMENDA√á√ïES

### Conclus√µes Prim√°rias:
"""
        
        for conclusion in self.master_report['conclusions_and_recommendations']['primary_conclusions']:
            text_report += f"‚Ä¢ {conclusion}\n"
        
        text_report += f"""

### Avalia√ß√£o de Confian√ßa:
{self.master_report['conclusions_and_recommendations']['confidence_assessment']['overall_assessment']}

### Limita√ß√µes:
"""
        
        for limitation in self.master_report['conclusions_and_recommendations']['limitations']:
            text_report += f"‚Ä¢ {limitation}\n"
        
        text_report += """

### Recomenda√ß√µes Imediatas:
"""
        
        for rec in self.master_report['conclusions_and_recommendations']['recommendations']['immediate_actions']:
            text_report += f"‚Ä¢ {rec}\n"
        
        text_report += """

### Pesquisa Futura:
"""
        
        for rec in self.master_report['conclusions_and_recommendations']['recommendations']['future_research']:
            text_report += f"‚Ä¢ {rec}\n"
        
        text_report += f"""

---

## IMPLICA√á√ïES PR√ÅTICAS

### Entendimento do Sistema:
"""
        
        for impl in self.master_report['practical_implications']['system_understanding']:
            text_report += f"‚Ä¢ {impl}\n"
        
        text_report += """

### Capacidade Preditiva:
"""
        
        for impl in self.master_report['practical_implications']['predictive_capability']:
            text_report += f"‚Ä¢ {impl}\n"
        
        text_report += """

### Considera√ß√µes de Seguran√ßa:
"""
        
        for impl in self.master_report['practical_implications']['security_considerations']:
            text_report += f"‚Ä¢ {impl}\n"
        
        text_report += f"""

---

**Relat√≥rio gerado pelo Sistema de An√°lise Abrangente Mega Sena v1.0.0**
**Timestamp: {self.timestamp}**
"""
        
        # Salvar relat√≥rio em texto
        text_path = self.output_dir / f"master_report_{self.timestamp}.txt"
        with open(text_path, 'w', encoding='utf-8') as f:
            f.write(text_report)
        
        print(f"‚úÖ Relat√≥rio em texto gerado: {text_path}")
    
    def create_interactive_visualizations(self):
        """Cria visualiza√ß√µes interativas com Plotly"""
        print("\nüìä FASE 9: Cria√ß√£o de Visualiza√ß√µes Interativas")
        print("-" * 50)
        
        try:
            self.create_comprehensive_dashboard()
            self.create_analysis_timeline()
            self.create_confidence_radar()
            self.create_prng_comparison()
            
            print("‚úÖ Visualiza√ß√µes interativas criadas com sucesso")
            
        except Exception as e:
            print(f"‚ùå Erro na cria√ß√£o de visualiza√ß√µes: {e}")
    
    def create_comprehensive_dashboard(self):
        """Cria dashboard abrangente"""
        print("üìà Criando dashboard abrangente...")
        
        # Criar subplots
        fig = make_subplots(
            rows=2, cols=2,
            subplot_titles=('Pontos de Mudan√ßa Detectados', 'Candidatos PRNG por Confian√ßa',
                          'Evolu√ß√£o dos Scores', 'Distribui√ß√£o de Confian√ßa'),
            specs=[[{'type': 'scatter'}, {'type': 'bar'}],
                   [{'type': 'scatter'}, {'type': 'histogram'}]]
        )
        
        # 1. Pontos de mudan√ßa
        consolidated = self.consolidated_results.get('consolidated', {})
        change_points = consolidated.get('all_change_points', [])
        
        if change_points:
            positions = [cp['position'] for cp in change_points]
            confidences = [cp['confidence'] for cp in change_points]
            sources = [cp['source'] for cp in change_points]
            
            fig.add_trace(
                go.Scatter(
                    x=positions,
                    y=confidences,
                    mode='markers+text',
                    text=[f"{src[:10]}" for src in sources],
                    textposition="top center",
                    marker=dict(size=10, color=confidences, colorscale='Viridis'),
                    name='Pontos de Mudan√ßa'
                ),
                row=1, col=1
            )
        
        # 2. Candidatos PRNG
        prng_candidates = consolidated.get('all_prng_candidates', [])
        
        if prng_candidates:
            prng_types = [c.get('type', 'unknown') for c in prng_candidates[:10]]
            prng_confidences = [c.get('confidence', 0) for c in prng_candidates[:10]]
            
            fig.add_trace(
                go.Bar(
                    x=prng_types,
                    y=prng_confidences,
                    marker_color=prng_confidences,
                    colorscale='Plasma',
                    name='PRNG Candidates'
                ),
                row=1, col=2
            )
        
        # 3. Evolu√ß√£o dos scores (se dispon√≠vel)
        solutions = consolidated.get('all_best_solutions', [])
        
        if solutions:
            solution_scores = [s.get('score', 0) for s in solutions[:20]]
            solution_indices = list(range(len(solution_scores)))
            
            fig.add_trace(
                go.Scatter(
                    x=solution_indices,
                    y=solution_scores,
                    mode='lines+markers',
                    line=dict(color='red', width=2),
                    name='Evolution of Scores'
                ),
                row=2, col=1
            )
        
        # 4. Distribui√ß√£o de confian√ßa
        all_confidences = []
        
        if change_points:
            all_confidences.extend([cp['confidence'] for cp in change_points])
        if prng_candidates:
            all_confidences.extend([c.get('confidence', 0) for c in prng_candidates])
        
        if all_confidences:
            fig.add_trace(
                go.Histogram(
                    x=all_confidences,
                    nbinsx=20,
                    marker_color='lightblue',
                    name='Confidence Distribution'
                ),
                row=2, col=2
            )
        
        # Layout
        fig.update_layout(
            title_text=f"Dashboard Abrangente - An√°lise Mega Sena ({self.timestamp})",
            height=800,
            showlegend=False
        )
        
        # Salvar
        dashboard_path = self.output_dir / f"comprehensive_dashboard_{self.timestamp}.html"
        fig.write_html(str(dashboard_path))
        
        print(f"   ‚úì Dashboard salvo em: {dashboard_path}")
    
    def create_analysis_timeline(self):
        """Cria timeline da an√°lise"""
        print("‚è±Ô∏è Criando timeline da an√°lise...")
        
        # Dados para timeline
        timeline_data = []
        
        # Pontos de mudan√ßa
        consolidated = self.consolidated_results.get('consolidated', {})
        change_points = consolidated.get('all_change_points', [])
        
        for cp in change_points:
            timeline_data.append({
                'x': cp['position'],
                'y': 1,
                'type': 'Change Point',
                'source': cp['source'],
                'confidence': cp['confidence']
            })
        
        # Dados de sorteios (amostra)
        sample_draws = self.historical_data[::50]  # Cada 50¬∫ sorteio
        for i, draw in enumerate(sample_draws):
            timeline_data.append({
                'x': i * 50,
                'y': 0,
                'type': 'Draw Sample',
                'source': f"Concurso {draw['concurso']}",
                'confidence': 1.0
            })
        
        # Criar figura
        fig = px.scatter(
            timeline_data,
            x='x',
            y='y',
            color='type',
            size='confidence',
            hover_data=['source', 'confidence'],
            title=f"Timeline da An√°lise - {len(self.historical_data)} Sorteios"
        )
        
        fig.update_layout(
            xaxis_title="Posi√ß√£o no Tempo (Sorteios)",
            yaxis_title="Tipo de Evento",
            height=400
        )
        
        # Salvar
        timeline_path = self.output_dir / f"analysis_timeline_{self.timestamp}.html"
        fig.write_html(str(timeline_path))
        
        print(f"   ‚úì Timeline salvo em: {timeline_path}")
    
    def create_confidence_radar(self):
        """Cria gr√°fico radar de confian√ßa"""
        print("üéØ Criando radar de confian√ßa...")
        
        # Dados para radar
        categories = [
            'Pontos de Mudan√ßa',
            'Detec√ß√£o PRNG',
            'Otimiza√ß√£o',
            'Correla√ß√£o Cruzada',
            'Valida√ß√£o Estat√≠stica'
        ]
        
        values = [
            self.confidence_scores.get('change_points', 0),
            self.confidence_scores.get('prng_detection', 0),
            self.confidence_scores.get('optimization', 0),
            0.5,  # Placeholder para correla√ß√£o cruzada
            0.6   # Placeholder para valida√ß√£o estat√≠stica
        ]
        
        # Criar figura radar
        fig = go.Figure()
        
        fig.add_trace(go.Scatterpolar(
            r=values,
            theta=categories,
            fill='toself',
            name='Scores de Confian√ßa',
            line=dict(color='blue', width=2),
            fillcolor='rgba(0, 100, 255, 0.2)'
        ))
        
        fig.update_layout(
            polar=dict(
                radialaxis=dict(
                    visible=True,
                    range=[0, 1]
                )),
            title=f"Radar de Confian√ßa - Score Geral: {self.confidence_scores.get('overall', 0):.3f}",
            height=500
        )
        
        # Salvar
        radar_path = self.output_dir / f"confidence_radar_{self.timestamp}.html"
        fig.write_html(str(radar_path))
        
        print(f"   ‚úì Radar de confian√ßa salvo em: {radar_path}")
    
    def create_prng_comparison(self):
        """Cria compara√ß√£o de candidatos PRNG"""
        print("üîß Criando compara√ß√£o de PRNGs...")
        
        consolidated = self.consolidated_results.get('consolidated', {})
        prng_candidates = consolidated.get('all_prng_candidates', [])
        
        if not prng_candidates:
            print("   ‚ö†Ô∏è Nenhum candidato PRNG para visualizar")
            return
        
        # Preparar dados
        comparison_data = []
        
        for candidate in prng_candidates[:10]:  # Top 10
            comparison_data.append({
                'Type': candidate.get('type', 'unknown'),
                'Confidence': candidate.get('confidence', 0),
                'Source': candidate.get('analysis_source', 'unknown'),
                'Method': candidate.get('method', 'N/A')
            })
        
        if not comparison_data:
            print("   ‚ö†Ô∏è Dados insuficientes para compara√ß√£o")
            return
        
        # Criar figura
        fig = px.bar(
            comparison_data,
            x='Type',
            y='Confidence',
            color='Source',
            hover_data=['Method'],
            title="Compara√ß√£o de Candidatos PRNG",
            text='Confidence'
        )
        
        fig.update_traces(texttemplate='%{text:.3f}', textposition='outside')
        fig.update_layout(
            xaxis_title="Tipo de PRNG",
            yaxis_title="Confian√ßa",
            height=500
        )
        
        # Salvar
        comparison_path = self.output_dir / f"prng_comparison_{self.timestamp}.html"
        fig.write_html(str(comparison_path))
        
        print(f"   ‚úì Compara√ß√£o PRNG salva em: {comparison_path}")
    
    def generate_final_recommendations(self):
        """Gera recomenda√ß√µes finais e pr√≥ximos passos"""
        print("\nüéØ FASE 10: Gera√ß√£o de Recomenda√ß√µes Finais")
        print("-" * 50)
        
        # An√°lise final baseada em todos os resultados
        final_analysis = self.perform_final_analysis()
        
        # Recomenda√ß√µes estrat√©gicas
        strategic_recommendations = self.generate_strategic_recommendations(final_analysis)
        
        # Plano de a√ß√£o
        action_plan = self.create_action_plan(final_analysis)
        
        # Salvar recomenda√ß√µes finais
        final_recommendations = {
            'timestamp': datetime.now().isoformat(),
            'final_analysis': final_analysis,
            'strategic_recommendations': strategic_recommendations,
            'action_plan': action_plan,
            'execution_summary': self.generate_execution_summary()
        }
        
        # Salvar
        recommendations_path = self.output_dir / f"final_recommendations_{self.timestamp}.json"
        with open(recommendations_path, 'w') as f:
            json.dump(final_recommendations, f, indent=2, default=str)
        
        print(f"‚úÖ Recomenda√ß√µes finais salvas em: {recommendations_path}")
        
        # Exibir resumo das recomenda√ß√µes
        self.display_final_summary(final_analysis, strategic_recommendations)
        
        return final_recommendations
    
    def perform_final_analysis(self):
        """Realiza an√°lise final consolidada"""
        consolidated = self.consolidated_results.get('consolidated', {})
        confidence = self.confidence_scores
        
        # Classificar n√≠vel de descoberta
        overall_confidence = confidence.get('overall', 0)
        
        if overall_confidence > 0.7:
            discovery_level = 'High'
            discovery_description = 'Padr√µes determin√≠sticos claros detectados'
        elif overall_confidence > 0.4:
            discovery_level = 'Medium'
            discovery_description = 'Padr√µes parciais detectados, requer valida√ß√£o'
        else:
            discovery_level = 'Low'
            discovery_description = 'Padr√µes m√≠nimos ou sistema verdadeiramente aleat√≥rio'
        
        # Analisar consist√™ncia entre m√©todos
        change_points = consolidated.get('all_change_points', [])
        prng_candidates = consolidated.get('all_prng_candidates', [])
        solutions = consolidated.get('all_best_solutions', [])
        
        consistency_score = self.calculate_method_consistency()
        
        return {
            'discovery_level': discovery_level,
            'discovery_description': discovery_description,
            'overall_confidence': overall_confidence,
            'consistency_score': consistency_score,
            'key_findings': {
                'change_points_count': len(change_points),
                'top_prng_candidate': prng_candidates[0] if prng_candidates else None,
                'best_solution_score': solutions[0]['score'] if solutions else 0
            },
            'method_performance': {
                'temporal_analysis': confidence.get('change_points', 0),
                'quantum_analysis': confidence.get('prng_detection', 0) * 0.5,  # Ponderado
                'reverse_engineering': confidence.get('prng_detection', 0) * 0.5,
                'genetic_optimization': confidence.get('optimization', 0)
            }
        }
    
    def calculate_method_consistency(self):
        """Calcula consist√™ncia entre m√©todos"""
        consolidated = self.consolidated_results.get('consolidated', {})
        correlations = consolidated.get('cross_correlations', {})
        
        consistency_factors = []
        
        # Consist√™ncia nos pontos de mudan√ßa
        change_correlation = correlations.get('change_points', {})
        if change_correlation:
            avg_correlation = np.mean(list(change_correlation.values()))
            consistency_factors.append(avg_correlation)
        
        # Consist√™ncia nos candidatos PRNG
        prng_correlation = correlations.get('prng_candidates', {})
        if prng_correlation and 'type_consensus' in prng_correlation:
            type_consensus = prng_correlation['type_consensus']
            if type_consensus:
                multi_source_types = sum(1 for sources in type_consensus.values() if len(sources) > 1)
                consensus_rate = multi_source_types / len(type_consensus)
                consistency_factors.append(consensus_rate)
        
        return np.mean(consistency_factors) if consistency_factors else 0
    
    def generate_strategic_recommendations(self, final_analysis):
        """Gera recomenda√ß√µes estrat√©gicas"""
        discovery_level = final_analysis['discovery_level']
        
        if discovery_level == 'High':
            return {
                'priority': 'immediate_validation',
                'recommendations': [
                    'Implementar sistema de monitoramento cont√≠nuo',
                    'Validar descobertas com dados independentes',
                    'Considerar implica√ß√µes de seguran√ßa',
                    'Documentar metodologia para auditoria',
                    'Desenvolver sistema de predi√ß√£o refinado'
                ],
                'resources_required': 'Altos - equipe especializada',
                'timeline': '1-3 meses'
            }
        elif discovery_level == 'Medium':
            return {
                'priority': 'continued_research',
                'recommendations': [
                    'Ampliar an√°lise com mais dados',
                    'Refinar m√©todos de detec√ß√£o',
                    'Buscar valida√ß√£o com outros datasets',
                    'Investigar m√©todos alternativos',
                    'Colaborar com especialistas externos'
                ],
                'resources_required': 'M√©dios - pesquisa continuada',
                'timeline': '3-6 meses'
            }
        else:
            return {
                'priority': 'alternative_approaches',
                'recommendations': [
                    'Explorar metodologias alternativas',
                    'Investigar fatores externos',
                    'Analisar outros sistemas de loteria',
                    'Desenvolver m√©todos mais sens√≠veis',
                    'Considerar an√°lise de hardware'
                ],
                'resources_required': 'Baixos - pesquisa explorat√≥ria',
                'timeline': '6-12 meses'
            }
    
    def create_action_plan(self, final_analysis):
        """Cria plano de a√ß√£o detalhado"""
        discovery_level = final_analysis['discovery_level']
        
        base_plan = {
            'immediate_actions': [
                'Documentar todos os resultados obtidos',
                'Criar backup de dados e c√≥digo',
                'Preparar apresenta√ß√£o dos resultados'
            ],
            'short_term': [
                'Validar resultados com pr√≥ximos sorteios',
                'Refinar par√¢metros mais promissores',
                'Buscar peer review da metodologia'
            ],
            'medium_term': [
                'Implementar melhorias baseadas em feedback',
                'Expandir an√°lise para outros contextos',
                'Desenvolver ferramentas automatizadas'
            ],
            'long_term': [
                'Estabelecer protocolo de monitoramento',
                'Publicar resultados em venues apropriados',
                'Colaborar com institui√ß√µes relevantes'
            ]
        }
        
        # Personalizar baseado no n√≠vel de descoberta
        if discovery_level == 'High':
            base_plan['immediate_actions'].append('Alertar stakeholders relevantes')
            base_plan['short_term'].append('Implementar sistema de valida√ß√£o rigoroso')
        elif discovery_level == 'Medium':
            base_plan['short_term'].append('Buscar dados adicionais para confirma√ß√£o')
            base_plan['medium_term'].append('Desenvolver m√©todos mais robustos')
        
        return base_plan
    
    def generate_execution_summary(self):
        """Gera resumo da execu√ß√£o"""
        return {
            'modules_executed': len(self.consolidated_results),
            'successful_analyses': len([r for r in self.consolidated_results.values() 
                                      if r.get('status') == 'completed']),
            'data_processed': len(self.historical_data),
            'outputs_generated': [
                'Relat√≥rio Mestre JSON',
                'Relat√≥rio Mestre Texto',
                'Dashboard Interativo',
                'Visualiza√ß√µes Espec√≠ficas',
                'Recomenda√ß√µes Finais'
            ],
            'key_metrics': {
                'overall_confidence': self.confidence_scores.get('overall', 0),
                'change_points_detected': len(self.consolidated_results.get('consolidated', {}).get('all_change_points', [])),
                'prng_candidates_found': len(self.consolidated_results.get('consolidated', {}).get('all_prng_candidates', [])),
                'optimization_solutions': len(self.consolidated_results.get('consolidated', {}).get('all_best_solutions', []))
            }
        }
    
    def display_final_summary(self, final_analysis, strategic_recommendations):
        """Exibe resumo final no console"""
        print("\n" + "="*80)
        print("üéØ RESUMO FINAL DA AN√ÅLISE MESTRE")
        print("="*80)
        
        print(f"\nüìä N√çVEL DE DESCOBERTA: {final_analysis['discovery_level'].upper()}")
        print(f"üìù Descri√ß√£o: {final_analysis['discovery_description']}")
        print(f"üéØ Confian√ßa Geral: {final_analysis['overall_confidence']:.3f}")
        print(f"üîó Consist√™ncia entre M√©todos: {final_analysis['consistency_score']:.3f}")
        
        print(f"\nüîç PRINCIPAIS DESCOBERTAS:")
        key_findings = final_analysis['key_findings']
        print(f"   ‚Ä¢ Pontos de mudan√ßa detectados: {key_findings['change_points_count']}")
        
        if key_findings['top_prng_candidate']:
            top_prng = key_findings['top_prng_candidate']
            print(f"   ‚Ä¢ Melhor candidato PRNG: {top_prng.get('type', 'N/A')} (confian√ßa: {top_prng.get('confidence', 0):.3f})")
        
        print(f"   ‚Ä¢ Melhor score de otimiza√ß√£o: {key_findings['best_solution_score']:.6f}")
        
        print(f"\nüìà PERFORMANCE DOS M√âTODOS:")
        performance = final_analysis['method_performance']
        for method, score in performance.items():
            print(f"   ‚Ä¢ {method.replace('_', ' ').title()}: {score:.3f}")
        
        print(f"\nüéØ PRIORIDADE ESTRAT√âGICA: {strategic_recommendations['priority'].upper()}")
        print(f"‚è±Ô∏è Timeline: {strategic_recommendations['timeline']}")
        print(f"üìä Recursos: {strategic_recommendations['resources_required']}")
        
        print(f"\nüìã PRINCIPAIS RECOMENDA√á√ïES:")
        for rec in strategic_recommendations['recommendations'][:3]:
            print(f"   ‚Ä¢ {rec}")
        
        print(f"\nüìÇ ARQUIVOS GERADOS:")
        print(f"   ‚Ä¢ Relat√≥rio Mestre: master_report_{self.timestamp}.json")
        print(f"   ‚Ä¢ Relat√≥rio Texto: master_report_{self.timestamp}.txt") 
        print(f"   ‚Ä¢ Dashboard: comprehensive_dashboard_{self.timestamp}.html")
        print(f"   ‚Ä¢ Recomenda√ß√µes: final_recommendations_{self.timestamp}.json")
        
        print("\n" + "="*80)
        print("‚úÖ AN√ÅLISE MESTRE ABRANGENTE CONCLU√çDA COM SUCESSO!")
        print("="*80)

# Script de execu√ß√£o principal
if __name__ == "__main__":
    print("üéØ SISTEMA DE AN√ÅLISE MESTRE ABRANGENTE")
    print("="*80)
    
    try:
        # Inicializar analisador mestre
        data_path = "../data/MegaSena3.xlsx"
        master_analyzer = ComprehensiveMasterAnalyzer(data_path)
        
        # Executar an√°lise completa
        master_analyzer.execute_comprehensive_analysis()
        
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è An√°lise interrompida pelo usu√°rio")
    except Exception as e:
        print(f"\n\n‚ùå Erro cr√≠tico: {e}")
        import traceback
        traceback.print_exc()
    
    print("\nüîö Execu√ß√£o finalizada.")